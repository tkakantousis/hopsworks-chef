#!/usr/bin/env bash

help() {
  echo "usage: $0 localWorkdir srcFileInHdfs IMPERSONATED_USERNAME"
  exit 1
}

if [ $# -ne 3 ] ; then
  help
fi

workdir=$1
hdfsFile="$2"
IMPERSONATED_USERNAME=$3
export HADOOP_USER_NAME="${IMPERSONATED_USERNAME}"

#FSM states: STAGING, UNZIPPING, UPLOADING, CHOWNING, SUCCESS, FAILED
fsmFile="${workdir}/fsm.txt"

# Sleep is needed, otherwise the user will not see final status (SUCCESS, FAILED) in UI
function finish {
    sleep 30s
    rm -rf "$workdir"
}
trap finish EXIT

function check_error {
    if [ $? -ne 0 ] ; then
      echo "FAILED" > "$fsmFile"
      exit 1	
    fi
}


prefix='hdfs://'
normalizedPath=${hdfsFile#${prefix}}
check_error

file=$(basename "$normalizedPath")
check_error
dir=$(dirname "$normalizedPath")
check_error

cd "$workdir"
check_error

filename=$(basename "$hdfsFile")

echo "STAGING" > "$fsmFile"
<%= node['hops']['base_dir'] %>/bin/hdfs dfs -copyToLocal "$hdfsFile" $workdir
check_error

echo "UNZIPPING" > "$fsmFile"

# State Machine State Change. File now staged to local FS.
# Java program checks if this file is written to see if this file exists:  $workdir/$(basename $hdfsfile)


# http://manpages.ubuntu.com/manpages/trusty/en/man1/dtrx.1.html
# , everything will be written to  a  dedicated  directory  that's named  after  the  archive.
# dtrx will also change the permissions to  ensure that the owner can read and write all those files.


cd "$workdir"
<%= @dtrx %> -o -n "$file"
check_error

# State Machine State Change. File now extracted to local FS.
# Java program checks if this file is written to see if this directory exists:  $workdir/$(basename $hdfsfile)


filedir="${file%%.*}"

case "$file" in 
  *.tar.gz)
     filedir=$(basename "$hdfsFile" .tar.gz)
    ;;
  *.bz2)
     filedir=$(basename "$hdfsFile" .bz2)
    ;;
  *.bzip2)
     filedir=$(basename "$hdfsFile" .bz2)
    ;;
  *.tgz)
     filedir=$(basename "$hdfsFile" .tgz)
    ;;
  *.gz)
     filedir=$(basename "$hdfsFile" .gz)
    ;;
  *.7z)
     filedir=$(basename "$hdfsFile" .7z)
    ;;
  *.rar)
     filedir=$(basename "$hdfsFile" .rar)
    ;;
  *.zip)
     filedir=$(basename "$hdfsFile" .zip)
    ;;
  *)
    help
    ;;
esac


#replace any spaces in the directory name with '_'
#Hadoop doesnt like spaces in paths.
escapedFiledir=${filedir// /_}
mv "$filedir" "$escapedFiledir"

echo "UPLOADING" > "$fsmFile"
<%= node['hops']['base_dir'] %>/bin/hdfs dfs -copyFromLocal "$escapedFiledir" "$dir"
check_error

echo "CHOWNING" > "$fsmFile"
<%= node['hops']['base_dir'] %>/bin/hdfs dfs -chown -R "${IMPERSONATED_USERNAME}" "$dir"/"$escapedFiledir"
check_error

echo "SUCCESS" > "$fsmFile"

